{"cells":[{"cell_type":"markdown","metadata":{"id":"1KKJLoUvCS7t"},"source":["### G-drive mounting and imports"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18814,"status":"ok","timestamp":1681405006963,"user":{"displayName":"Alex Daffara","userId":"00842358211352088658"},"user_tz":240},"id":"LdVJuifJCWi6","outputId":"cb17365a-099b-4248-cc87-4fc89f00dd9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive\n","fatal: destination path 'yolov7' already exists and is not an empty directory.\n","/content/drive/MyDrive/yolov7\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# download yolo repo to G-Drive\n","%cd /content/drive/MyDrive\n","!git clone https://github.com/WongKinYiu/yolov7\n","\n","%cd /content/drive/MyDrive/yolov7\n","# install Requirements\n","# !pip install -r requirements.txt \n","\n","# download yolov7 starting weights checkpoint\n","# !wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n","\n","# IMPORTS\n","# !pip install ar\n","# import ar"]},{"cell_type":"markdown","metadata":{"id":"bHfT9gEiBsBd"},"source":["# Custom Training 01\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4264212,"status":"ok","timestamp":1681412706098,"user":{"displayName":"Alex Daffara","userId":"00842358211352088658"},"user_tz":240},"id":"1iqOPKjr22mL","outputId":"86ce2fbe-8610-49d9-b519-0d3ef1275c50"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/yolov7\n","2023-04-13 17:54:03.945552: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-04-13 17:54:04.968447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","YOLOR ðŸš€ v0.1-122-g3b41c2c torch 2.0.0+cu118 CUDA:0 (Tesla T4, 15101.8125MB)\n","\n","Namespace(weights='yolov7.pt', cfg='cfg/training/custom_yolov7.yaml', data='data/waymo_data.yaml', hyp='data/hyp.scratch.custom.yaml', epochs=5, batch_size=16, img_size=[640, 640], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='0', multi_scale=False, single_cls=False, adam=False, sync_bn=False, local_rank=-1, workers=8, project='runs/train', entity=None, name='yolov7_master', exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', freeze=[0], v5_metric=False, world_size=1, global_rank=-1, save_dir='runs/train/yolov7_master10', total_batch_size=16)\n","\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, paste_in=0.0, loss_ota=1\n","\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n","  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n","  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 12                -1  1         0  models.common.MP                        []                            \n"," 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n"," 25                -1  1         0  models.common.MP                        []                            \n"," 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"," 38                -1  1         0  models.common.MP                        []                            \n"," 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n"," 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"," 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n"," 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n"," 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n"," 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n"," 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n"," 76                -1  1         0  models.common.MP                        []                            \n"," 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n"," 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n"," 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n"," 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n"," 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 89                -1  1         0  models.common.MP                        []                            \n"," 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n"," 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n"," 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n","100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n","101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n","102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n","103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n","104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n","105   [102, 103, 104]  1     39550  models.yolo.IDetect                     [2, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\n","Model Summary: 415 layers, 37201950 parameters, 37201950 gradients\n","\n","Transferred 552/566 items from yolov7.pt\n","Scaled weight_decay = 0.0005\n","Optimizer groups: 95 .bias, 95 conv.weight, 98 other\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'data/train_01/labels.cache' images and labels... 4629 found, 2364 missing, 0 empty, 16 corrupted: 100% 6993/6993 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning 'data/val/labels.cache' images and labels... 4013 found, 0 missing, 0 empty, 4 corrupted: 100% 4013/4013 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 2.51, Best Possible Recall (BPR) = 0.8781. Attempting to improve anchors, please wait...\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mWARNING: Extremely small objects found. 13750 of 170076 labels are < 3 pixels in size.\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 9 anchors on 169720 points...\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9981 best possible recall, 4.36 anchors past thr\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.294/0.715-mean/best, past_thr=0.477-mean: 6,5,  7,12,  15,10,  12,26,  29,19,  21,52,  56,34,  98,72,  171,159\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7348: 100% 1000/1000 [00:41<00:00, 24.06it/s]\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9987 best possible recall, 4.89 anchors past thr\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.321/0.734-mean/best, past_thr=0.484-mean: 5,4,  5,9,  10,8,  7,19,  18,14,  15,35,  36,25,  70,48,  145,132\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n","\n","Image sizes 640 train, 640 test\n","Using 4 dataloader workers\n","Logging results to runs/train/yolov7_master10\n","Starting training for 5 epochs...\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       0/4     1.41G   0.06483   0.02378  0.005534   0.09415        56       640: 100% 437/437 [11:58<00:00,  1.64s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:   0% 0/126 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 126/126 [02:01<00:00,  1.03it/s]\n","                 all        4009      134985       0.585       0.417       0.419       0.153\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       1/4     13.9G   0.05046   0.02341  0.002068   0.07594         7       640: 100% 437/437 [11:45<00:00,  1.61s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 126/126 [02:01<00:00,  1.04it/s]\n","                 all        4009      134985       0.549       0.397       0.414       0.173\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       2/4     14.2G   0.04451   0.02283  0.001381   0.06871         0       640: 100% 437/437 [11:48<00:00,  1.62s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 126/126 [02:02<00:00,  1.03it/s]\n","                 all        4009      134985       0.611         0.4       0.441       0.196\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       3/4     14.2G   0.03966   0.02171 0.0009326    0.0623        38       640: 100% 437/437 [11:42<00:00,  1.61s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 126/126 [01:59<00:00,  1.06it/s]\n","                 all        4009      134985       0.669       0.403       0.463       0.223\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       4/4     14.2G   0.03859   0.02148 0.0007761   0.06084        12       640: 100% 437/437 [11:45<00:00,  1.61s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 126/126 [02:16<00:00,  1.08s/it]\n","                 all        4009      134985       0.649       0.414       0.469        0.23\n","             vehicle        4009      102216        0.69       0.463       0.527       0.278\n","              person        4009       32769       0.607       0.364       0.412       0.182\n","5 epochs completed in 1.167 hours.\n","\n","Optimizer stripped from runs/train/yolov7_master10/weights/last.pt, 74.8MB\n","Optimizer stripped from runs/train/yolov7_master10/weights/best.pt, 74.8MB\n"]}],"source":["# train model\n","%cd /content/drive/MyDrive/yolov7\n","!python train.py --device 0 --batch-size 16 --epochs 5 --img 640 640 --data data/waymo_data.yaml --hyp data/hyp.scratch.custom.yaml --cfg cfg/training/custom_yolov7.yaml --weights yolov7.pt --name yolov7_master\n"]},{"cell_type":"markdown","metadata":{"id":"jw0TdwEt_1QI"},"source":["# Custom Training 23\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"858fa0f2-75c4-4299-dbfa-9684ec9ab203","collapsed":true,"id":"o_bx8Xcq_1QJ","executionInfo":{"status":"ok","timestamp":1681415454480,"user_tz":240,"elapsed":2540087,"user":{"displayName":"Alex Daffara","userId":"00842358211352088658"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/yolov7\n","2023-04-13 19:08:36.270750: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-04-13 19:08:37.294292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","YOLOR ðŸš€ v0.1-122-g3b41c2c torch 2.0.0+cu118 CUDA:0 (Tesla T4, 15101.8125MB)\n","\n","Namespace(weights='runs/train/yolov7_master10/weights/last.pt', cfg='cfg/training/custom_yolov7.yaml', data='data/waymo_data_23.yaml', hyp='data/hyp.scratch.custom.yaml', epochs=5, batch_size=16, img_size=[640, 640], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='0', multi_scale=False, single_cls=False, adam=False, sync_bn=False, local_rank=-1, workers=8, project='runs/train', entity=None, name='yolov7_master', exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', freeze=[0], v5_metric=False, world_size=1, global_rank=-1, save_dir='runs/train/yolov7_master12', total_batch_size=16)\n","\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, paste_in=0.0, loss_ota=1\n","\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n","  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n","  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n","  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 12                -1  1         0  models.common.MP                        []                            \n"," 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n"," 25                -1  1         0  models.common.MP                        []                            \n"," 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"," 38                -1  1         0  models.common.MP                        []                            \n"," 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n"," 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n"," 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n"," 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n"," 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n"," 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n"," 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n"," 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n"," 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n"," 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n"," 76                -1  1         0  models.common.MP                        []                            \n"," 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n"," 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n"," 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n"," 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n"," 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n"," 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n"," 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n"," 89                -1  1         0  models.common.MP                        []                            \n"," 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n"," 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n"," 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n"," 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n"," 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n"," 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n","100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n","101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n","102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n","103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n","104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n","105   [102, 103, 104]  1     39550  models.yolo.IDetect                     [2, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\n","Model Summary: 415 layers, 37201950 parameters, 37201950 gradients\n","\n","Transferred 564/566 items from runs/train/yolov7_master10/weights/last.pt\n","Scaled weight_decay = 0.0005\n","Optimizer groups: 95 .bias, 95 conv.weight, 98 other\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'data/train_23/labels' images and labels... 257 found, 0 missing, 0 empty, 0 corrupted:  11% 257/2242 [02:10<10:54,  3.03it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label data/train_23/images/13363977648531075793_343_000_363_000;1525983364698572.jpg: duplicate labels\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'data/train_23/labels' images and labels... 276 found, 0 missing, 0 empty, 1 corrupted:  12% 276/2242 [02:16<11:00,  2.97it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label data/train_23/images/13363977648531075793_343_000_363_000;1525983368499249.jpg: duplicate labels\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'data/train_23/labels' images and labels... 329 found, 0 missing, 0 empty, 2 corrupted:  15% 329/2242 [02:34<14:07,  2.26it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label data/train_23/images/13402473631986525162_5700_000_5720_000;1557326774847960.jpg: duplicate labels\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'data/train_23/labels' images and labels... 361 found, 0 missing, 0 empty, 3 corrupted:  16% 361/2242 [02:45<10:25,  3.01it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label data/train_23/images/13402473631986525162_5700_000_5720_000;1557326780147575.jpg: duplicate labels\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'data/train_23/labels' images and labels... 1176 found, 0 missing, 0 empty, 4 corrupted:  52% 1176/2242 [07:11<05:30,  3.23it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label data/train_23/images/13840133134545942567_1060_000_1080_000;1558402363663661.jpg: duplicate labels\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'data/train_23/labels' images and labels... 1350 found, 0 missing, 0 empty, 5 corrupted:  60% 1350/2242 [08:08<04:57,  3.00it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label data/train_23/images/13944915979337652825_4260_668_4280_668;1508604820075853.jpg: duplicate labels\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'data/train_23/labels' images and labels... 2242 found, 0 missing, 0 empty, 6 corrupted: 100% 2242/2242 [12:57<00:00,  2.88it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: data/train_23/labels.cache\n","\u001b[34m\u001b[1mval: \u001b[0mScanning 'data/val/labels.cache' images and labels... 4013 found, 0 missing, 0 empty, 4 corrupted: 100% 4013/4013 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 2.34, Best Possible Recall (BPR) = 0.8437. Attempting to improve anchors, please wait...\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mWARNING: Extremely small objects found. 7826 of 78070 labels are < 3 pixels in size.\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 9 anchors on 77838 points...\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9982 best possible recall, 4.29 anchors past thr\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.293/0.715-mean/best, past_thr=0.482-mean: 5,4,  8,9,  17,11,  10,21,  31,20,  20,46,  56,36,  97,70,  158,132\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7453: 100% 1000/1000 [00:28<00:00, 34.87it/s]\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 0.9997 best possible recall, 5.01 anchors past thr\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.331/0.744-mean/best, past_thr=0.488-mean: 4,4,  7,5,  6,11,  13,9,  18,15,  12,30,  33,24,  54,40,  122,95\n","\u001b[34m\u001b[1mautoanchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n","\n","Image sizes 640 train, 640 test\n","Using 4 dataloader workers\n","Logging results to runs/train/yolov7_master12\n","Starting training for 5 epochs...\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       0/4      2.7G   0.04065   0.02894 0.0007514   0.07034       593       640: 100% 140/140 [04:04<00:00,  1.74s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95:   0% 0/126 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 126/126 [02:00<00:00,  1.05it/s]\n","                 all        4009      134985       0.719       0.455       0.522       0.265\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       1/4     14.2G    0.0383    0.0266 0.0006215   0.06553       285       640: 100% 140/140 [03:40<00:00,  1.57s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 126/126 [01:53<00:00,  1.11it/s]\n","                 all        4009      134985       0.724       0.498       0.563       0.291\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       2/4     13.3G   0.03792   0.02618 0.0006088   0.06471       566       640: 100% 140/140 [03:33<00:00,  1.53s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 126/126 [01:52<00:00,  1.12it/s]\n","                 all        4009      134985       0.737       0.505       0.573       0.298\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       3/4     13.3G   0.03753   0.02659 0.0005735    0.0647       499       640: 100% 140/140 [03:30<00:00,  1.50s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 126/126 [01:48<00:00,  1.17it/s]\n","                 all        4009      134985       0.725       0.495       0.563       0.292\n","\n","     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n","       4/4     13.3G   0.03708   0.02573 0.0005487   0.06335       418       640: 100% 140/140 [03:27<00:00,  1.48s/it]\n","               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 126/126 [02:06<00:00,  1.00s/it]\n","                 all        4009      134985       0.722        0.51       0.571       0.298\n","             vehicle        4009      102216       0.732       0.558       0.611       0.346\n","              person        4009       32769       0.712       0.463       0.531        0.25\n","5 epochs completed in 0.475 hours.\n","\n","Optimizer stripped from runs/train/yolov7_master12/weights/last.pt, 74.8MB\n","Optimizer stripped from runs/train/yolov7_master12/weights/best.pt, 74.8MB\n"]}],"source":["# train model\n","%cd /content/drive/MyDrive/yolov7\n","!python train.py --device 0 --batch-size 16 --epochs 5 --img 640 640 --data data/waymo_data_23.yaml --hyp data/hyp.scratch.custom.yaml --cfg cfg/training/custom_yolov7.yaml --weights runs/train/yolov7_master10/weights/last.pt --name yolov7_master\n"]},{"cell_type":"markdown","metadata":{"id":"QUsPwRO4BIX2"},"source":["# Custom Training 45\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"76da8a49-868c-4894-842d-fb4d818eae33","collapsed":true,"id":"qgCQwxKFBIX2","executionInfo":{"status":"ok","timestamp":1681415463986,"user_tz":240,"elapsed":8366,"user":{"displayName":"Alex Daffara","userId":"00842358211352088658"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/yolov7\n","2023-04-13 19:50:57.432976: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-04-13 19:50:58.439848: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","YOLOR ðŸš€ v0.1-122-g3b41c2c torch 2.0.0+cu118 CUDA:0 (Tesla T4, 15101.8125MB)\n","\n","Namespace(weights='runs/train/yolov7_master_12/weights/last.pt', cfg='cfg/training/custom_yolov7.yaml', data='data/waymo_data_45.yaml', hyp='data/hyp.scratch.custom.yaml', epochs=5, batch_size=16, img_size=[640, 640], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='0', multi_scale=False, single_cls=False, adam=False, sync_bn=False, local_rank=-1, workers=8, project='runs/train', entity=None, name='yolov7_master', exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', freeze=[0], v5_metric=False, world_size=1, global_rank=-1, save_dir='runs/train/yolov7_master13', total_batch_size=16)\n","\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, paste_in=0.0, loss_ota=1\n","\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/yolov7/train.py\", line 616, in <module>\n","    train(hyp, opt, device, tb_writer)\n","  File \"/content/drive/MyDrive/yolov7/train.py\", line 87, in train\n","    ckpt = torch.load(weights, map_location=device)  # load checkpoint\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 791, in load\n","    with _open_file_like(f, 'rb') as opened_file:\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 271, in _open_file_like\n","    return _open_file(name_or_buffer, mode)\n","  File \"/usr/local/lib/python3.9/dist-packages/torch/serialization.py\", line 252, in __init__\n","    super().__init__(open(name, mode))\n","FileNotFoundError: [Errno 2] No such file or directory: 'runs/train/yolov7_master_12/weights/last.pt'\n"]}],"source":["# train model\n","%cd /content/drive/MyDrive/yolov7\n","!python train.py --device 0 --batch-size 16 --epochs 5 --img 640 640 --data data/waymo_data_45.yaml --hyp data/hyp.scratch.custom.yaml --cfg cfg/training/custom_yolov7.yaml --weights runs/train/yolov7_master12/weights/last.pt --name yolov7_master\n"]},{"cell_type":"markdown","metadata":{"id":"0W0MpUaTCJro"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20755,"status":"ok","timestamp":1681255265714,"user":{"displayName":"Alex Daffara","userId":"00842358211352088658"},"user_tz":240},"id":"N4cfnLtTCIce","outputId":"55db5d95-d055-4e40-ff13-4996a6f01952"},"outputs":[{"name":"stdout","output_type":"stream","text":["Namespace(weights=['runs/train/yolov7_100_imgs/weights/best.pt'], source='data/test/images', img_size=640, conf_thres=0.35, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n","YOLOR ðŸš€ v0.1-122-g3b41c2c torch 2.0.0+cu118 CUDA:0 (Tesla T4, 15101.8125MB)\n","\n","Fusing layers... \n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","IDetect.fuse\n","Model Summary: 314 layers, 36487166 parameters, 6194944 gradients\n"," Convert model to Traced-model... \n"," traced_script_module saved! \n"," model is traced! \n","\n","/usr/local/lib/python3.9/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","1 vehicle, Done. (16.4ms) Inference, (1.5ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/10964956617027590844_1584_680_1604_680;1508973309382846.jpg\n","3 vehicles, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/10975280749486260148_940_000_960_000;1521206976925688.jpg\n","Done. (16.5ms) Inference, (0.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11004685739714500220_2300_000_2320_000;1553640296967706.jpg\n","4 vehicles, Done. (16.6ms) Inference, (1.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11017034898130016754_697_830_717_830;1507646787248340.jpg\n","Done. (16.5ms) Inference, (0.7ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11060291335850384275_3761_210_3781_210;1521853091055933.jpg\n","13 vehicles, 18 persons, Done. (16.6ms) Inference, (1.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11070802577416161387_740_000_760_000;1557159314647380.jpg\n","3 vehicles, Done. (16.6ms) Inference, (1.5ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11076364019363412893_1711_000_1731_000;1520989135194969.jpg\n","5 vehicles, 11 persons, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11113047206980595400_2560_000_2580_000;1557855070247056.jpg\n","3 vehicles, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11119453952284076633_1369_940_1389_940;1509393509384759.jpg\n","3 vehicles, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11126313430116606120_1439_990_1459_990;1506961341666238.jpg\n","1 vehicle, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11139647661584646830_5470_000_5490_000;1518656974137730.jpg\n","1 vehicle, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11183906854663518829_2294_000_2314_000;1520782154837677.jpg\n","9 vehicles, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11199484219241918646_2810_030_2830_030;1507664616676990.jpg\n","6 vehicles, Done. (16.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11219370372259322863_5320_000_5340_000;1543361399222016.jpg\n","5 vehicles, 18 persons, Done. (16.6ms) Inference, (1.6ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11236550977973464715_3620_000_3640_000;1557962956115170.jpg\n","9 vehicles, 7 persons, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11252086830380107152_1540_000_1560_000;1543280778337424.jpg\n","16 vehicles, 6 persons, Done. (16.6ms) Inference, (1.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11318901554551149504_520_000_540_000;1557338092401764.jpg\n","4 vehicles, Done. (16.5ms) Inference, (1.6ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11343624116265195592_5910_530_5930_530;1521839105685642.jpg\n","8 vehicles, Done. (16.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11355519273066561009_5323_000_5343_000;1518656827188821.jpg\n","1 person, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11379226583756500423_6230_810_6250_810;1507665518203613.jpg\n","14 vehicles, 3 persons, Done. (16.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11388947676680954806_5427_320_5447_320;1507250081684091.jpg\n","14 vehicles, Done. (16.6ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11392401368700458296_1086_429_1106_429;1509139846725056.jpg\n","2 vehicles, Done. (16.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11454085070345530663_1905_000_1925_000;1522612750019771.jpg\n","14 vehicles, 6 persons, Done. (16.6ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/1146261869236413282_1680_000_1700_000;1558401155147376.jpg\n","3 vehicles, Done. (16.5ms) Inference, (1.6ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11486225968269855324_92_000_112_000;1520094349399557.jpg\n","4 vehicles, 1 person, Done. (16.5ms) Inference, (1.3ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11489533038039664633_4820_000_4840_000;1543275788406880.jpg\n","8 vehicles, Done. (16.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11566385337103696871_5740_000_5760_000;1549564670298935.jpg\n","6 vehicles, Done. (16.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11588853832866011756_2184_462_2204_462;1509041568305289.jpg\n","3 vehicles, Done. (16.5ms) Inference, (1.7ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11623618970700582562_2840_367_2860_367;1513626610403419.jpg\n","21 vehicles, 6 persons, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11674150664140226235_680_000_700_000;1546474406347549.jpg\n","13 vehicles, 3 persons, Done. (16.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11718898130355901268_2300_000_2320_000;1558017581297237.jpg\n","6 vehicles, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/1172406780360799916_1660_000_1680_000;1553526154125776.jpg\n","12 vehicles, Done. (16.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11799592541704458019_9828_750_9848_750;1506959828427730.jpg\n","9 vehicles, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11839652018869852123_2565_000_2585_000;1522862349349153.jpg\n","27 vehicles, Done. (16.5ms) Inference, (1.5ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11846396154240966170_3540_000_3560_000;1553820967473534.jpg\n","19 vehicles, 4 persons, Done. (16.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11847506886204460250_1640_000_1660_000;1557450193447575.jpg\n","8 vehicles, 1 person, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/1191788760630624072_3880_000_3900_000;1553811890198707.jpg\n","10 vehicles, Done. (16.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11918003324473417938_1400_000_1420_000;1544817128712621.jpg\n","19 vehicles, 17 persons, Done. (16.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11925224148023145510_1040_000_1060_000;1558141171073901.jpg\n","20 vehicles, 1 person, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11928449532664718059_1200_000_1220_000;1557363974537692.jpg\n","18 vehicles, 15 persons, Done. (16.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11940460932056521663_1760_000_1780_000;1558483917222033.jpg\n","13 vehicles, 12 persons, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11967272535264406807_580_000_600_000;1557873612698864.jpg\n","9 vehicles, 4 persons, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/11971497357570544465_1200_000_1220_000;1549293258472998.jpg\n","22 vehicles, Done. (16.6ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/12012663867578114640_820_000_840_000;1553908859299093.jpg\n","12 vehicles, 1 person, Done. (16.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/12027892938363296829_4086_280_4106_280;1510081607621572.jpg\n","3 vehicles, Done. (16.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/1208303279778032257_1360_000_1380_000;1553525854105205.jpg\n","19 vehicles, 1 person, Done. (16.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/12161824480686739258_1813_380_1833_380;1510614091939092.jpg\n","8 vehicles, Done. (16.5ms) Inference, (1.5ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/12174529769287588121_3848_440_3868_440;1510691443123157.jpg\n","7 vehicles, Done. (16.5ms) Inference, (1.1ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/12179768245749640056_5561_070_5581_070;1507996723183980.jpg\n","4 vehicles, Done. (16.5ms) Inference, (1.2ms) NMS\n"," The image with the result is saved in: runs/detect/exp3/12200383401366682847_2552_140_2572_140;1521778168941077.jpg\n","Done. (6.175s)\n"]}],"source":["# Run evaluation\n","!python detect.py --weights runs/train/yolov7_100_imgs/weights/best.pt --conf-thres 0.35 --source data/test/images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1W6U2cZ8RyMEzEUtw086133abS8sxnGEe"},"executionInfo":{"elapsed":131034,"status":"error","timestamp":1681262090915,"user":{"displayName":"Alex Daffara","userId":"00842358211352088658"},"user_tz":240},"id":"XkHxEZhFBXV1","outputId":"9d8bcf2f-b4d7-4b75-b78f-9639006c41b8"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["#display inference on ALL test images\n","\n","import glob\n","from IPython.display import Image, display\n","\n","i = 0\n","limit = 10000 # max images to print\n","for imageName in glob.glob('/content/drive/MyDrive/yolov7/runs/detect/exp4/*.jpg'): #assuming JPG\n","    if i < limit:\n","      display(Image(filename=imageName))\n","      print(\"\\n\")\n","    i = i + 1\n","    "]},{"cell_type":"markdown","metadata":{"id":"aMumI7a2JDAN"},"source":["# Reparameterize for Inference\n","\n","https://github.com/WongKinYiu/yolov7/blob/main/tools/reparameterization.ipynb"]},{"cell_type":"markdown","metadata":{"id":"4jn4kCtgKiGO"},"source":["# OPTIONAL: Deployment\n","\n","To deploy, you'll need to export your weights and save them to use later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWOok8abrCsL"},"outputs":[],"source":["# optional, zip to download weights and results locally\n","\n","!zip -r export.zip runs/detect\n","!zip -r export.zip runs/train/exp/weights/best.pt\n","!zip export.zip runs/train/exp/*"]},{"cell_type":"markdown","metadata":{"id":"f41PvE5gKhYw"},"source":["# OPTIONAL: Active Learning Example\n","\n","Once our first training run is complete, we should use our model to help identify which images are most problematic in order to investigate, annotate, and improve our dataset (and, therefore, model).\n","\n","To do that, we can execute code that automatically uploads images back to our hosted dataset if the image is a specific class or below a given confidence threshold.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mcINqQS7Kt3-"},"outputs":[],"source":["# # setup access to your workspace\n","# rf = Roboflow(api_key=\"YOUR_API_KEY\")                               # used above to load data\n","# inference_project =  rf.workspace().project(\"YOUR_PROJECT_NAME\")    # used above to load data\n","# model = inference_project.version(1).model\n","\n","# upload_project = rf.workspace().project(\"YOUR_PROJECT_NAME\")\n","\n","# print(\"inference reference point: \", inference_project)\n","# print(\"upload destination: \", upload_project)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cEl1NVE3LSD_"},"outputs":[],"source":["# # example upload: if prediction is below a given confidence threshold, upload it \n","\n","# confidence_interval = [10,70]                                   # [lower_bound_percent, upper_bound_percent]\n","\n","# for prediction in predictions:                                  # predictions list to loop through\n","#   if(prediction['confidence'] * 100 >= confidence_interval[0] and \n","#           prediction['confidence'] * 100 <= confidence_interval[1]):\n","        \n","#           # upload on success!\n","#           print(' >> image uploaded!')\n","#           upload_project.upload(image, num_retry_uploads=3)     # upload image in question"]},{"cell_type":"markdown","metadata":{"id":"LVpCFeU-K4gb"},"source":["# Next steps\n","\n","Congratulations, you've trained a custom YOLOv7 model! Next, start thinking about deploying and [building an MLOps pipeline](https://docs.roboflow.com) so your model gets better the more data it sees in the wild."]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-object-detection-on-custom-data.ipynb","timestamp":1681230280060}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}